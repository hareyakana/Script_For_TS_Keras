{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Using Convolution Filters with clustering techniques (DBSCAN) and Understanding of CNN via layer wise relevance propagation (or similar techniques) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 26 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 5.0, 4.0\n",
    "\n",
    "from pyts.transformation import GADF,GASF\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import uproot\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard(x,height,decay):\n",
    "    y = height*np.exp(-x*decay)\n",
    "    return y\n",
    "\n",
    "def shifter(x,starts):\n",
    "    L = int(starts*len(x))\n",
    "    y = np.zeros(len(x))\n",
    "    y[:L] = np.zeros(L) \n",
    "    y[L:] = x[:(len(x)-L)]\n",
    "    return y\n",
    "\n",
    "def comb_standard(x,second,height_1,decay_1,height_2,decay_2):\n",
    "    L = int(second*len(x))\n",
    "    y = np.zeros(len(x))\n",
    "    y[:L] = standard(x[:L],height_1,decay_1)\n",
    "    y[L:] = standard(x[:(len(x)-L)],height_2,decay_2)\n",
    "    return y\n",
    "\n",
    "def noiser(x,strength):\n",
    "    y = x + np.random.normal(0,strength,len(x))\n",
    "    return y\n",
    "\n",
    "def noiser_long(x,strength):\n",
    "    noise = np.random.normal(0,strength,len(x))\n",
    "    y = x + np.cumsum(noise)*strength\n",
    "    return y\n",
    "\n",
    "def noiser_comb(x,sepfact,strength):\n",
    "    L = int(sepfact*len(x))\n",
    "    x[:L] = noiser(x[:L],strength)\n",
    "    x[L:] = noiser_long(x[L:],strength)\n",
    "    return x\n",
    "\n",
    "def array_maker(entries):\n",
    "    x = np.arange(0,1,1/4096)\n",
    "    x = np.expand_dims(x,axis=0)\n",
    "    x = np.tile(x,[entries,1])\n",
    "    return x\n",
    "\n",
    "def double(x,second,height_1,decay_1,height_2,decay_2,starts,sepfact=0.15,strength=0.02):\n",
    "    y = comb_standard(x,second,height_1,decay_1,height_2,decay_2)\n",
    "    y = shifter(y,starts)\n",
    "    y = noiser_comb(y,sepfact,strength)\n",
    "    return y\n",
    "\n",
    "def single(x,height,decay,starts,sepfact=0.15,strength=0.02):\n",
    "    y = standard(x,height,decay)\n",
    "    y = shifter(y,starts)\n",
    "    y = noiser_comb(y,sepfact,strength)\n",
    "    return y\n",
    "\n",
    "def event_creators_single(entries):\n",
    "    x = array_maker(entries)\n",
    "    for i in range(entries):\n",
    "        w = np.random.normal(1,0.01)\n",
    "        r = np.random.normal(5,2)\n",
    "        s = np.random.normal(0.03,0.005)\n",
    "        x[i] = single(x[i],w,r,s)\n",
    "    return x\n",
    "\n",
    "def event_creators_single_2(entries):\n",
    "    x = array_maker(entries)\n",
    "    for i in range(entries):\n",
    "        w = np.random.normal(1,0.01)\n",
    "        r = np.random.normal(10,2)\n",
    "        s = np.random.normal(0.03,0.005)\n",
    "        x[i] = single(x[i],w,r,s)\n",
    "    return x\n",
    "\n",
    "def event_creators_single_3(entries):\n",
    "    x = array_maker(entries)\n",
    "    for i in range(entries):\n",
    "        w = np.random.normal(1,0.1)\n",
    "        r = np.random.normal(4,1)\n",
    "        s = np.random.normal(0.03,0.005)\n",
    "        x[i] = single(x[i],w,r,s)\n",
    "    return x\n",
    "\n",
    "def event_creators_sharp(entries):\n",
    "    x = array_maker(entries)\n",
    "    for i in range(entries):\n",
    "        w = np.random.normal(1,0.01)\n",
    "        r = np.random.normal(100,20)\n",
    "        s = np.random.normal(0.03,0.005)\n",
    "        x[i] = single(x[i],w,r,s)    \n",
    "    return x\n",
    "\n",
    "def event_creators_double_equal(entries):\n",
    "    x = array_maker(entries)\n",
    "    for i in range(entries):\n",
    "        w = np.random.normal(0.2,0.01)\n",
    "        p = np.random.normal(1,0.01)\n",
    "        q = np.random.normal(5,2)\n",
    "        r = np.random.normal(1,0.01)\n",
    "        s = np.random.normal(5,2)\n",
    "        t = np.random.normal(0.03,0.005)\n",
    "        x[i] = double(x[i],w,p,q,r,s,t)\n",
    "    return x\n",
    "    \n",
    "def event_creators_double_unequal(entries):\n",
    "    x = array_maker(entries)\n",
    "    for i in range(entries):\n",
    "        w = np.random.normal(0.2,0.01)\n",
    "        p = np.random.normal(1,0.2)\n",
    "        q = np.random.normal(50,10)\n",
    "        r = np.random.normal(1,0.2)\n",
    "        s = np.random.normal(50,10)\n",
    "        t = np.random.normal(0.03,0.005)\n",
    "        x[i] = double(x[i],w,p,q,r,s,t)\n",
    "    return x\n",
    "\n",
    "def event_creators_sharp_fat(entries):\n",
    "    x = array_maker(entries)\n",
    "    for i in range(entries):\n",
    "        w = np.random.normal(0.015,0.001)\n",
    "        p = np.random.normal(1,0.2)\n",
    "        q = np.random.normal(100,20)\n",
    "        r = np.random.normal(0.2,0.05)\n",
    "        s = np.random.normal(5,2)\n",
    "        t = np.random.normal(0.03,0.005)\n",
    "        x[i] = double(x[i],w,p,q,r,s,t)\n",
    "    return x\n",
    "\n",
    "def label(q,k):\n",
    "    x = np.zeros(len(q))\n",
    "    for i in range(len(q)):\n",
    "        x[i] = k\n",
    "    return x\n",
    "\n",
    "def sep(q,k,z):\n",
    "    y = label(q,k)\n",
    "    x1, x2 ,x3 = np.split(q,[int(len(q)*training_ratio),int(len(q)*(training_ratio+validation_ratio))])\n",
    "    y1, y2 ,y3 = np.split(y,[int(len(q)*training_ratio),int(len(q)*(training_ratio+validation_ratio))])\n",
    "    if z == 0:\n",
    "        return x1, y1\n",
    "    if z == 1:\n",
    "        return x2, y2\n",
    "    if z == 2:\n",
    "        return x3, y3\n",
    "\n",
    "def reader_pmtall(path):\n",
    "    extra = np.arange(4096, 4480)\n",
    "    tree = uproot.open(path)[\"tree\"]\n",
    "    pmtall = tree.array(\"PMTALL\")\n",
    "    pmtall = np.delete(pmtall, extra, axis=1)\n",
    "    return pmtall\n",
    "\n",
    "def reader(path,branch,number):\n",
    "    tree = uproot.open(path)[\"tree\"]\n",
    "    column = tree.array(branch)\n",
    "    column = column[:,number]\n",
    "    return column\n",
    "\n",
    "def reader_lone(path,branch):\n",
    "    tree = uproot.open(path)[\"tree\"]\n",
    "    column = tree.array(branch)\n",
    "    return column\n",
    "\n",
    "def pmtall_pedestal(path):\n",
    "    pedestal = reader(path,\"Pedestal\",0)\n",
    "    pmtall = reader_pmtall(path)\n",
    "    for i in range(len(pedestal)):\n",
    "        pmtall[i] = -(pmtall[i]-pedestal[i])\n",
    "    \n",
    "    return pmtall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb(one,two,three,four,five,six,seven,eight,nine,ten,portion):\n",
    "    one1,one2 = sep(one,1,portion)\n",
    "    two1,two2 = sep(two,1,portion)\n",
    "    three1,three2 = sep(three,1,portion)\n",
    "    four1,four2 = sep(four,1,portion)\n",
    "    five1,five2 = sep(five,1,portion)\n",
    "    six1,six2 = sep(six,0,portion)\n",
    "    seven1,seven2 = sep(seven,0,portion)\n",
    "    eight1,eight2 = sep(eight,0,portion)\n",
    "    nine1,nine2 = sep(nine,0,portion)\n",
    "    ten1,ten2 = sep(ten,0,portion)\n",
    "\n",
    "    z = np.concatenate((one1,two1,three1,four1,five1,six1,seven1,eight1,nine1,ten1),axis=0)\n",
    "    y = np.concatenate((one2,two2,three2,four2,five2,six2,seven2,eight2,nine2,ten2),axis=0)\n",
    "    return z, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "build it using python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "class Waveform():\n",
    "    \n",
    "    def __init__(self, path=None, no_classes=None):\n",
    "        if path is None:\n",
    "            raise ValueError(\"Insert file path!\")\n",
    "        if no_classes is None:\n",
    "            raise ValueError(\"Number of classes?\")\n",
    "        \n",
    "        # Load PMTALL(sum of waveform of CANDLES), removing last portion of data\n",
    "        tree = uproot.open(path)[\"tree\"]\n",
    "        extra = np.arange(4096,4480)\n",
    "        pmtall = tree.array(\"PMTALL\")\n",
    "        pmtall = np.delete(pmtall, extra, axis=1)\n",
    "        pedestal = tree.array(\"Pedestal\")\n",
    "        pedestal_sum = pedestal[:,0]\n",
    "        for i in range(len(pedestal_sum)):\n",
    "            pmtall[i] = -(pmtall[i]-pedestal_sum[i])\n",
    "#         number = \n",
    "        \n",
    "        # random labelling(test purposes)\n",
    "        self.waveform = pmtall\n",
    "        self.label = np.random.randint(3,size=(len(pmtall),))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.waveform.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return (self.waveform[idx],self.label[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_classes = 3\n",
    "dataset = Waveform(path=\"Run009-069-001.root\", no_classes=no_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-599a9dceaa76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m data_loader = DataLoader(dataset=dataset,\n\u001b[0m\u001b[1;32m      4\u001b[0m                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1000\n",
    "from torch.utils.data import DataLoader\n",
    "data_loader = DataLoader(dataset=dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=True,\n",
    "                         num_workers=3) \n",
    "\n",
    "# #Testing#\n",
    "# for i in dataset:\n",
    "#     print(len(i[0]),len(i[1]))\n",
    "# for i in data_loader:\n",
    "#     print(i[0].size())\n",
    "#     print(i[1].size())\n",
    "# print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = int(len(dataset)/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define CNN structure\n",
    "using an autoencoder for self-training, taking encoder part or decoder part for features learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.1732 -0.7867  1.0059  0.7242 -0.6296 -1.8647  1.3830 -0.6172 -0.6004 -0.6850\n",
       "-1.1482 -0.0940 -2.4120  1.2823  0.7159 -0.1950  1.1712 -0.2942 -0.1067 -0.0765\n",
       " 1.1541  0.4790 -2.3716  1.7382  1.4308  1.0778  0.7979 -1.0894 -1.2398  0.3669\n",
       "-1.1194  1.5223  0.0146 -1.9548  0.3726 -1.4361  1.4112 -0.9027  1.6328 -0.3982\n",
       "-1.6377 -0.5356 -0.2465  0.2279  1.3486 -1.0220  0.1333  1.2906  1.4574  1.0001\n",
       "-0.8595  0.5142 -0.2984 -1.0140  1.1616  0.0935 -0.7162  0.0328  1.7915  0.1084\n",
       " 1.2633  1.6165  1.4476  0.6378 -2.6928  0.2502 -1.4982 -0.1744  0.2629  2.0208\n",
       "-1.0208  1.5432 -0.3982 -0.9259  0.6875 -1.3311  0.5838 -0.0857 -1.2087 -1.2809\n",
       " 0.4147 -0.5013  3.0467  1.7507  2.9731  1.2949  0.2020  0.6395 -0.0952  1.1100\n",
       " 0.2361 -1.5246  0.2510  0.7203  0.4728  0.3298 -0.5078 -2.0981  1.1358  1.2134\n",
       "[torch.FloatTensor of size 10x10]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (c1): Conv1d(1, 32, kernel_size=(8,), stride=(1,), padding=(4,))\n",
      "  (p1): MaxPool1d(kernel_size=2, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (lr): LeakyReLU(0.2)\n",
      "  (c2): Conv1d(32, 16, kernel_size=(8,), stride=(1,), padding=(4,))\n",
      "  (p2): MaxPool1d(kernel_size=2, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (l1): Linear(in_features=4096, out_features=64, bias=True)\n",
      "  (out): Linear(in_features=64, out_features=3, bias=True)\n",
      "  (sg): Sigmoid()\n",
      ")\n",
      "torch.Size([32, 1, 8])\n",
      "torch.Size([32])\n",
      "torch.Size([16, 32, 8])\n",
      "torch.Size([16])\n",
      "torch.Size([64, 4096])\n",
      "torch.Size([64])\n",
      "torch.Size([3, 64])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Discriminator\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_chn, no_classes, kernel_size, no_filters, padding, maxpool, batch_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.input_chn = input_chn\n",
    "        self.no_classes = no_classes\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.no_filters = no_filters\n",
    "        self.maxpool = maxpool\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.c1 = nn.Conv1d(input_chn ,no_filters ,kernel_size ,padding=padding )\n",
    "        self.p1 = nn.MaxPool1d(maxpool ,padding )\n",
    "        self.lr = nn.LeakyReLU(0.2)\n",
    "        self.c2 = nn.Conv1d(no_filters, int(no_filters/2), kernel_size, padding=padding)\n",
    "        self.p2 = nn.MaxPool1d(maxpool ,padding )       \n",
    "        self.l1 = nn.Linear(4096,64)\n",
    "        self.out = nn.Linear(64, no_classes)  \n",
    "        self.sg = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        x = inputs.view(batch_size,1,-1)\n",
    "        x = self.c1(inputs)\n",
    "        x = self.lr(x)\n",
    "        x = self.p1(x)\n",
    "        x = self.c2(x)\n",
    "        x = self.lr(x)\n",
    "        x = self.p2(x)\n",
    "        \n",
    "        x = x.view(self.batch_size,-1)\n",
    "        \n",
    "        x = self.l1(x)\n",
    "        x = self.lr(x)\n",
    "        x = self.out(x)\n",
    "        outputs = self.sg(x)\n",
    "        return outputs\n",
    "\n",
    "CNN = Classifier(1, no_classes=3, kernel_size=8, no_filters=32, padding=4, maxpool=2, batch_size=BATCH_SIZE) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(CNN.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "print(CNN)\n",
    "for parameter in CNN.parameters():\n",
    "    print(parameter.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training\n",
    "transform to Torch.Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def to_var(x):\n",
    "    # first move to GPU, if necessary\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy code workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Variable(torch.randn(10,120))\n",
    "target = Variable(torch.FloatTensor(10).uniform_(0, 120).long())\n",
    "print(torch.FloatTensor(10))\n",
    "print(torch.FloatTensor(10).uniform_(0,1).long())\n",
    "# print(output,target)\n",
    "\n",
    "loss = criterion(output,target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/2], Step[5/41], loss=1.1120, Mean Discriminator output=0.6957\n",
      "Epoch[0/2], Step[10/41], loss=1.1186, Mean Discriminator output=0.7564\n",
      "Epoch[0/2], Step[15/41], loss=1.0974, Mean Discriminator output=0.8304\n",
      "Epoch[0/2], Step[20/41], loss=1.0963, Mean Discriminator output=0.8430\n",
      "Epoch[0/2], Step[25/41], loss=1.1000, Mean Discriminator output=0.8614\n",
      "Epoch[0/2], Step[30/41], loss=1.1011, Mean Discriminator output=0.8904\n",
      "Epoch[0/2], Step[35/41], loss=1.1005, Mean Discriminator output=0.9178\n",
      "Epoch[0/2], Step[40/41], loss=1.0983, Mean Discriminator output=0.9508\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: size '[1000 x -1]' is invalid for input with 1064960 elements at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/THStorage.c:37",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b4859a50b0e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#             print(training_data,target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m#             print(outputs, target.view(-1).long())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d365f097007d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: size '[1000 x -1]' is invalid for input with 1064960 elements at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/THStorage.c:37"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 2\n",
    "\n",
    "# allow for manual keyboard interrupt\n",
    "try: \n",
    "    # loop through epochs\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        for batch_number, (waveform,label) in enumerate(data_loader):\n",
    "            \n",
    "#             print(\"epoch=\",epoch)\n",
    "#             print(batch_number)\n",
    "#             print(waveform.size(),label.size())\n",
    "    \n",
    "            batch_size = waveform.size()[0]\n",
    "            training_data = to_var(waveform.view(batch_size,1,4096))\n",
    "            target = to_var(label.view(batch_size,-1))\n",
    "            \n",
    "#             print(training_data,target)\n",
    "            \n",
    "            outputs = CNN(training_data)\n",
    "#             print(outputs, target.view(-1).long())\n",
    "            \n",
    "            real_score = outputs\n",
    "            loss = criterion(outputs, target.view(-1).long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             print(loss.data[0])\n",
    "        \n",
    "            if (batch_number +1)%5 == 0:\n",
    "                print(\"Epoch[%d/%d], Step[%d/%d], loss=%.4f, Mean Discriminator output=%.4f\"\n",
    "                      %(epoch,\n",
    "                        N_EPOCHS,\n",
    "                        batch_number+1,\n",
    "                        n_batches,\n",
    "                        loss.data[0],outputs.data.mean()))\n",
    "            \n",
    "            torch.save(CNN.state_dict(), \"CNN.pkl\")\n",
    "        \n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('Training ended early.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing output of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-da180dbfa514>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## PLOT the test results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d365f097007d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "test = CNN(torch.randn((100,4096)))\n",
    "\n",
    "## PLOT the test results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extracting filters/features\n",
    "\n",
    "Use the protion of the Autoencoder transfer the trained weight to semi-half model of autoencoder.\n",
    "Uses the output of the midlle layer of autoencoder and do a clustering on it.\n",
    "\n",
    "The cluster number is a single vector(multi-class in keras?) representing the \"output\" of the group class.\n",
    "\n",
    "# How to let this last layer of output automatically deduce itself.?\n",
    "* save every sample output? how to make gives a supervised like methods output??\n",
    "\n",
    "#Retrain the network but freeze all layer except for the last layer (clustering analysis layer). this can uses for LWRP studies, identify physical meaning of grouping. with it?\n",
    "\n",
    "* understanding Neural Network, how to get layer-wise relevance propagation work for the last layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering techniques\n",
    "most likely DBSCAN don't require to specify number of clusters however this can explode making difficult to use, the best option for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Wise Relevance Propagation Or similiar techniques\n",
    "The purpose of understanding CNN, see which portion of data gives more importance, mostly likely create a new CNN using trained weights from autoencoder with the final layers self-created based of the results of clustering. This can allow us to treate this CNN as supervised technique like however in reality as it is based on purely unsupervised techniques. This supervised like technique allows us to use technique like layer wise relevance propagation to understand the CNN giving us insight of the working of features learned by the cNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
