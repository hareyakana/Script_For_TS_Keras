{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Using Convolution Filters with clustering techniques (DBSCAN) and Understanding of CNN via layer wise relevance propagation (or similar techniques) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 5.0, 4.0\n",
    "\n",
    "from pyts.transformation import GADF,GASF\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import uproot\n",
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard(x,height,decay):\n",
    "    y = height*np.exp(-x*decay)\n",
    "    return y\n",
    "\n",
    "def shifter(x,starts):\n",
    "    L = int(starts*len(x))\n",
    "    y = np.zeros(len(x))\n",
    "    y[:L] = np.zeros(L) \n",
    "    y[L:] = x[:(len(x)-L)]\n",
    "    return y\n",
    "\n",
    "def comb_standard(x,second,height_1,decay_1,height_2,decay_2):\n",
    "    L = int(second*len(x))\n",
    "    y = np.zeros(len(x))\n",
    "    y[:L] = standard(x[:L],height_1,decay_1)\n",
    "    y[L:] = standard(x[:(len(x)-L)],height_2,decay_2)\n",
    "    return y\n",
    "\n",
    "def noiser(x,strength):\n",
    "    y = x + np.random.normal(0,strength,len(x))\n",
    "    return y\n",
    "\n",
    "def noiser_long(x,strength):\n",
    "    noise = np.random.normal(0,strength,len(x))\n",
    "    y = x + np.cumsum(noise)*strength\n",
    "    return y\n",
    "\n",
    "def noiser_comb(x,sepfact,strength):\n",
    "    L = int(sepfact*len(x))\n",
    "    x[:L] = noiser(x[:L],strength)\n",
    "    x[L:] = noiser_long(x[L:],strength)\n",
    "    return x\n",
    "\n",
    "def array_maker(entries):\n",
    "    x = np.arange(0,1,1/4096)\n",
    "    x = np.expand_dims(x,axis=0)\n",
    "    x = np.tile(x,[entries,1])\n",
    "    return x\n",
    "\n",
    "def double(x,second,height_1,decay_1,height_2,decay_2,starts,sepfact=0.15,strength=0.02):\n",
    "    y = comb_standard(x,second,height_1,decay_1,height_2,decay_2)\n",
    "    y = shifter(y,starts)\n",
    "    y = noiser_comb(y,sepfact,strength)\n",
    "    return y\n",
    "\n",
    "def single(x,height,decay,starts,sepfact=0.15,strength=0.02):\n",
    "    y = standard(x,height,decay)\n",
    "    y = shifter(y,starts)\n",
    "    y = noiser_comb(y,sepfact,strength)\n",
    "    return y\n",
    "\n",
    "def event_creators_single(entries):\n",
    "    x = array_maker(entries)\n",
    "    for i in range(entries):\n",
    "        w = np.random.normal(1,0.01)\n",
    "        r = np.random.normal(5,2)\n",
    "        s = np.random.normal(0.03,0.005)\n",
    "        x[i] = single(x[i],w,r,s)\n",
    "    return x\n",
    "\n",
    "def event_creators_single_2(entries):\n",
    "    x = array_maker(entries)\n",
    "    for i in range(entries):\n",
    "        w = np.random.normal(1,0.01)\n",
    "        r = np.random.normal(10,2)\n",
    "        s = np.random.normal(0.03,0.005)\n",
    "        x[i] = single(x[i],w,r,s)\n",
    "    return x\n",
    "\n",
    "def event_creators_single_3(entries):\n",
    "    x = array_maker(entries)\n",
    "    for i in range(entries):\n",
    "        w = np.random.normal(1,0.1)\n",
    "        r = np.random.normal(4,1)\n",
    "        s = np.random.normal(0.03,0.005)\n",
    "        x[i] = single(x[i],w,r,s)\n",
    "    return x\n",
    "\n",
    "def event_creators_sharp(entries):\n",
    "    x = array_maker(entries)\n",
    "    for i in range(entries):\n",
    "        w = np.random.normal(1,0.01)\n",
    "        r = np.random.normal(100,20)\n",
    "        s = np.random.normal(0.03,0.005)\n",
    "        x[i] = single(x[i],w,r,s)    \n",
    "    return x\n",
    "\n",
    "def event_creators_double_equal(entries):\n",
    "    x = array_maker(entries)\n",
    "    for i in range(entries):\n",
    "        w = np.random.normal(0.2,0.01)\n",
    "        p = np.random.normal(1,0.01)\n",
    "        q = np.random.normal(5,2)\n",
    "        r = np.random.normal(1,0.01)\n",
    "        s = np.random.normal(5,2)\n",
    "        t = np.random.normal(0.03,0.005)\n",
    "        x[i] = double(x[i],w,p,q,r,s,t)\n",
    "    return x\n",
    "    \n",
    "def event_creators_double_unequal(entries):\n",
    "    x = array_maker(entries)\n",
    "    for i in range(entries):\n",
    "        w = np.random.normal(0.2,0.01)\n",
    "        p = np.random.normal(1,0.2)\n",
    "        q = np.random.normal(50,10)\n",
    "        r = np.random.normal(1,0.2)\n",
    "        s = np.random.normal(50,10)\n",
    "        t = np.random.normal(0.03,0.005)\n",
    "        x[i] = double(x[i],w,p,q,r,s,t)\n",
    "    return x\n",
    "\n",
    "def event_creators_sharp_fat(entries):\n",
    "    x = array_maker(entries)\n",
    "    for i in range(entries):\n",
    "        w = np.random.normal(0.015,0.001)\n",
    "        p = np.random.normal(1,0.2)\n",
    "        q = np.random.normal(100,20)\n",
    "        r = np.random.normal(0.2,0.05)\n",
    "        s = np.random.normal(5,2)\n",
    "        t = np.random.normal(0.03,0.005)\n",
    "        x[i] = double(x[i],w,p,q,r,s,t)\n",
    "    return x\n",
    "\n",
    "def label(q,k):\n",
    "    x = np.zeros(len(q))\n",
    "    for i in range(len(q)):\n",
    "        x[i] = k\n",
    "    return x\n",
    "\n",
    "def sep(q,k,z):\n",
    "    y = label(q,k)\n",
    "    x1, x2 ,x3 = np.split(q,[int(len(q)*training_ratio),int(len(q)*(training_ratio+validation_ratio))])\n",
    "    y1, y2 ,y3 = np.split(y,[int(len(q)*training_ratio),int(len(q)*(training_ratio+validation_ratio))])\n",
    "    if z == 0:\n",
    "        return x1, y1\n",
    "    if z == 1:\n",
    "        return x2, y2\n",
    "    if z == 2:\n",
    "        return x3, y3\n",
    "\n",
    "def reader_pmtall(path):\n",
    "    extra = np.arange(4096, 4480)\n",
    "    tree = uproot.open(path)[\"tree\"]\n",
    "    pmtall = tree.array(\"PMTALL\")\n",
    "    pmtall = np.delete(pmtall, extra, axis=1)\n",
    "    return pmtall\n",
    "\n",
    "def reader(path,branch,number):\n",
    "    tree = uproot.open(path)[\"tree\"]\n",
    "    column = tree.array(branch)\n",
    "    column = column[:,number]\n",
    "    return column\n",
    "\n",
    "def reader_lone(path,branch):\n",
    "    tree = uproot.open(path)[\"tree\"]\n",
    "    column = tree.array(branch)\n",
    "    return column\n",
    "\n",
    "def pmtall_pedestal(path):\n",
    "    pedestal = reader(path,\"Pedestal\",0)\n",
    "    pmtall = reader_pmtall(path)\n",
    "    for i in range(len(pedestal)):\n",
    "        pmtall[i] = -(pmtall[i]-pedestal[i])\n",
    "    \n",
    "    return pmtall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comb(one,two,three,four,five,six,seven,eight,nine,ten,portion):\n",
    "    one1,one2 = sep(one,1,portion)\n",
    "    two1,two2 = sep(two,1,portion)\n",
    "    three1,three2 = sep(three,1,portion)\n",
    "    four1,four2 = sep(four,1,portion)\n",
    "    five1,five2 = sep(five,1,portion)\n",
    "    six1,six2 = sep(six,0,portion)\n",
    "    seven1,seven2 = sep(seven,0,portion)\n",
    "    eight1,eight2 = sep(eight,0,portion)\n",
    "    nine1,nine2 = sep(nine,0,portion)\n",
    "    ten1,ten2 = sep(ten,0,portion)\n",
    "\n",
    "    z = np.concatenate((one1,two1,three1,four1,five1,six1,seven1,eight1,nine1,ten1),axis=0)\n",
    "    y = np.concatenate((one2,two2,three2,four2,five2,six2,seven2,eight2,nine2,ten2),axis=0)\n",
    "    return z, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "build it using python class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "class Waveform():\n",
    "    \n",
    "    def __init__(self, path=None, no_classes=None):\n",
    "        if path is None:\n",
    "            raise ValueError(\"Insert file path!\")\n",
    "        if no_classes is None:\n",
    "            raise ValueError(\"Number of classes?\")\n",
    "        \n",
    "        # Load PMTALL(sum of waveform of CANDLES), removing last portion of data\n",
    "        tree = uproot.open(path)[\"tree\"]\n",
    "        extra = np.arange(4096,4480)\n",
    "        pmtall = tree.array(\"PMTALL\")\n",
    "        pmtall = np.delete(pmtall, extra, axis=1)\n",
    "        pedestal = tree.array(\"Pedestal\")\n",
    "        pedestal_sum = pedestal[:,0]\n",
    "        for i in range(len(pedestal_sum)):\n",
    "            pmtall[i] = -(pmtall[i]-pedestal_sum[i])\n",
    "#         number = \n",
    "        \n",
    "        # random labelling(test purposes)\n",
    "        self.waveform = pmtall\n",
    "        self.label = np.random.randint(3,size=(len(pmtall),))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.waveform.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return (self.waveform[idx],self.label[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_classes = 3\n",
    "dataset = Waveform(path=\"Run009-069-001.root\", no_classes=no_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "from torch.utils.data import DataLoader\n",
    "data_loader = DataLoader(dataset=dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=True,\n",
    "                         num_workers=3) \n",
    "\n",
    "# #Testing#\n",
    "# for i in dataset:\n",
    "#     print(len(i[0]),len(i[1]))\n",
    "# for i in data_loader:\n",
    "#     print(i[0].size())\n",
    "#     print(i[1].size())\n",
    "# print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = int(len(dataset)/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define CNN structure\n",
    "using an autoencoder for self-training, taking encoder part or decoder part for features learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4096, out_features=512, bias=True)\n",
      "  (1): LeakyReLU(0.2)\n",
      "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (3): LeakyReLU(0.2)\n",
      "  (4): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (5): LeakyReLU(0.2)\n",
      "  (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (7): LeakyReLU(0.2)\n",
      "  (8): Linear(in_features=64, out_features=3, bias=True)\n",
      "  (9): Sigmoid()\n",
      ")\n",
      "torch.Size([512, 4096])\n",
      "torch.Size([512])\n",
      "torch.Size([256, 512])\n",
      "torch.Size([256])\n",
      "torch.Size([64, 256])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([3, 64])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# Discriminator\n",
    "Discriminator = nn.Sequential(\n",
    "    nn.Linear(4096, 512),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(256, 64),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(64, no_classes),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(Discriminator.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "print(Discriminator)\n",
    "for parameter in Discriminator.parameters():\n",
    "    print(parameter.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training\n",
    "transform to Torch.Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def to_var(x):\n",
    "    # first move to GPU, if necessary\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy code workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0000e+00\n",
      " 0.0000e+00\n",
      " 1.9945e-28\n",
      "-3.6893e+19\n",
      " 4.2039e-45\n",
      " 9.8091e-45\n",
      " 4.3489e-25\n",
      " 1.4013e-45\n",
      " 0.0000e+00\n",
      " 0.0000e+00\n",
      "[torch.FloatTensor of size 10]\n",
      "\n",
      "\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.LongTensor of size 10]\n",
      "\n",
      "Variable containing:\n",
      " 5.0597\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = Variable(torch.randn(10,120))\n",
    "target = Variable(torch.FloatTensor(10).uniform_(0, 120).long())\n",
    "print(torch.FloatTensor(10))\n",
    "print(torch.FloatTensor(10).uniform_(0,1).long())\n",
    "# print(output,target)\n",
    "\n",
    "loss = criterion(output,target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/10], Step[5/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[0/10], Step[10/41], loss=1.0986, Mean Discriminator output=-0.0129\n",
      "Epoch[0/10], Step[15/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[0/10], Step[20/41], loss=1.0981, Mean Discriminator output=-0.0129\n",
      "Epoch[0/10], Step[25/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[0/10], Step[30/41], loss=1.0981, Mean Discriminator output=-0.0129\n",
      "Epoch[0/10], Step[35/41], loss=1.0986, Mean Discriminator output=-0.0129\n",
      "Epoch[0/10], Step[40/41], loss=1.0990, Mean Discriminator output=-0.0129\n",
      "Epoch[1/10], Step[5/41], loss=1.0981, Mean Discriminator output=-0.0129\n",
      "Epoch[1/10], Step[10/41], loss=1.0986, Mean Discriminator output=-0.0129\n",
      "Epoch[1/10], Step[15/41], loss=1.0995, Mean Discriminator output=-0.0129\n",
      "Epoch[1/10], Step[20/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[1/10], Step[25/41], loss=1.0986, Mean Discriminator output=-0.0129\n",
      "Epoch[1/10], Step[30/41], loss=1.0975, Mean Discriminator output=-0.0129\n",
      "Epoch[1/10], Step[35/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[1/10], Step[40/41], loss=1.0981, Mean Discriminator output=-0.0129\n",
      "Epoch[2/10], Step[5/41], loss=1.0981, Mean Discriminator output=-0.0129\n",
      "Epoch[2/10], Step[10/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[2/10], Step[15/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[2/10], Step[20/41], loss=1.0981, Mean Discriminator output=-0.0129\n",
      "Epoch[2/10], Step[25/41], loss=1.0981, Mean Discriminator output=-0.0129\n",
      "Epoch[2/10], Step[30/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[2/10], Step[35/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[2/10], Step[40/41], loss=1.0975, Mean Discriminator output=-0.0129\n",
      "Epoch[3/10], Step[5/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[3/10], Step[10/41], loss=1.0986, Mean Discriminator output=-0.0129\n",
      "Epoch[3/10], Step[15/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[3/10], Step[20/41], loss=1.0990, Mean Discriminator output=-0.0129\n",
      "Epoch[3/10], Step[25/41], loss=1.0990, Mean Discriminator output=-0.0129\n",
      "Epoch[3/10], Step[30/41], loss=1.0985, Mean Discriminator output=-0.0129\n",
      "Epoch[3/10], Step[35/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[3/10], Step[40/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[4/10], Step[5/41], loss=1.0981, Mean Discriminator output=-0.0129\n",
      "Epoch[4/10], Step[10/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[4/10], Step[15/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[4/10], Step[20/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[4/10], Step[25/41], loss=1.0981, Mean Discriminator output=-0.0129\n",
      "Epoch[4/10], Step[30/41], loss=1.0981, Mean Discriminator output=-0.0129\n",
      "Epoch[4/10], Step[35/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[4/10], Step[40/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[5/10], Step[5/41], loss=1.0986, Mean Discriminator output=-0.0129\n",
      "Epoch[5/10], Step[10/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[5/10], Step[15/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[5/10], Step[20/41], loss=1.0986, Mean Discriminator output=-0.0129\n",
      "Epoch[5/10], Step[25/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[5/10], Step[30/41], loss=1.0986, Mean Discriminator output=-0.0129\n",
      "Epoch[5/10], Step[35/41], loss=1.0986, Mean Discriminator output=-0.0129\n",
      "Epoch[5/10], Step[40/41], loss=1.0985, Mean Discriminator output=-0.0129\n",
      "Epoch[6/10], Step[5/41], loss=1.0980, Mean Discriminator output=-0.0129\n",
      "Epoch[6/10], Step[10/41], loss=1.0980, Mean Discriminator output=-0.0129\n",
      "Epoch[6/10], Step[15/41], loss=1.0985, Mean Discriminator output=-0.0129\n",
      "Epoch[6/10], Step[20/41], loss=1.0985, Mean Discriminator output=-0.0129\n",
      "Epoch[6/10], Step[25/41], loss=1.0995, Mean Discriminator output=-0.0129\n",
      "Epoch[6/10], Step[30/41], loss=1.0986, Mean Discriminator output=-0.0129\n",
      "Epoch[6/10], Step[35/41], loss=1.0995, Mean Discriminator output=-0.0129\n",
      "Epoch[6/10], Step[40/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[7/10], Step[5/41], loss=1.0975, Mean Discriminator output=-0.0129\n",
      "Epoch[7/10], Step[10/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[7/10], Step[15/41], loss=1.0985, Mean Discriminator output=-0.0129\n",
      "Epoch[7/10], Step[20/41], loss=1.0981, Mean Discriminator output=-0.0129\n",
      "Epoch[7/10], Step[25/41], loss=1.0986, Mean Discriminator output=-0.0129\n",
      "Epoch[7/10], Step[30/41], loss=1.0986, Mean Discriminator output=-0.0129\n",
      "Epoch[7/10], Step[35/41], loss=1.0986, Mean Discriminator output=-0.0129\n",
      "Epoch[7/10], Step[40/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[8/10], Step[5/41], loss=1.0986, Mean Discriminator output=-0.0129\n",
      "Epoch[8/10], Step[10/41], loss=1.0986, Mean Discriminator output=-0.0129\n",
      "Epoch[8/10], Step[15/41], loss=1.0985, Mean Discriminator output=-0.0129\n",
      "Epoch[8/10], Step[20/41], loss=1.0981, Mean Discriminator output=-0.0129\n",
      "Epoch[8/10], Step[25/41], loss=1.0986, Mean Discriminator output=-0.0129\n",
      "Epoch[8/10], Step[30/41], loss=1.0995, Mean Discriminator output=-0.0129\n",
      "Epoch[8/10], Step[35/41], loss=1.0985, Mean Discriminator output=-0.0129\n",
      "Epoch[8/10], Step[40/41], loss=1.0990, Mean Discriminator output=-0.0129\n",
      "Epoch[9/10], Step[5/41], loss=1.0986, Mean Discriminator output=-0.0129\n",
      "Epoch[9/10], Step[10/41], loss=1.0981, Mean Discriminator output=-0.0129\n",
      "Epoch[9/10], Step[15/41], loss=1.1000, Mean Discriminator output=-0.0129\n",
      "Epoch[9/10], Step[20/41], loss=1.0986, Mean Discriminator output=-0.0129\n",
      "Epoch[9/10], Step[25/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[9/10], Step[30/41], loss=1.0995, Mean Discriminator output=-0.0129\n",
      "Epoch[9/10], Step[35/41], loss=1.0991, Mean Discriminator output=-0.0129\n",
      "Epoch[9/10], Step[40/41], loss=1.0991, Mean Discriminator output=-0.0129\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "# allow for manual keyboard interrupt\n",
    "try: \n",
    "    # loop through epochs\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        for batch_number, (waveform,label) in enumerate(data_loader):\n",
    "            \n",
    "#             print(\"epoch=\",epoch)\n",
    "#             print(batch_number)\n",
    "#             print(waveform.size(),label.size())\n",
    "    \n",
    "            batch_size = waveform.size()[0]\n",
    "            training_data = to_var(waveform.view(batch_size,-1))\n",
    "            target = to_var(label.view(batch_size,-1))\n",
    "            \n",
    "#             print(training_data,training_label)\n",
    "            \n",
    "            outputs = Discriminator(training_data)\n",
    "#             print(outputs, target.view(-1).long())\n",
    "            \n",
    "            real_score = outputs\n",
    "            loss = criterion(outputs, target.view(-1).long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             print(loss.data[0])\n",
    "        \n",
    "            if (batch_number +1)%5 == 0:\n",
    "                print(\"Epoch[%d/%d], Step[%d/%d], loss=%.4f, Mean Discriminator output=%.4f\"\n",
    "                      %(epoch,\n",
    "                        N_EPOCHS,\n",
    "                        batch_number+1,\n",
    "                        n_batches,\n",
    "                        loss.data[0],output.data.mean()))\n",
    "        \n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('Training ended early.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extracting filters/features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering techniques\n",
    "most likely DBSCAN don't require to specify number of clusters however this can explode making difficult to use, the best option for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Wise Relevance Propagation Or similiar techniques\n",
    "The purpose of understanding CNN, see which portion of data gives more importance, mostly likely create a new CNN using trained weights from autoencoder with the final layers self-created based of the results of clustering. This can allow us to treate this CNN as supervised technique like however in reality as it is based on purely unsupervised techniques. This supervised like technique allows us to use technique like layer wise relevance propagation to understand the CNN giving us insight of the working of features learned by the cNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
